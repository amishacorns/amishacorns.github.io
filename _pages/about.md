---
permalink: /
title: "Summary"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Like many others, I enjoy exploring the fundamental unsolved questions of nature, including the foundations of consciousness, quantitative ethical frameworks, and most promising recipes for general artificial intelligence. We are currently within the most dynamic and exciting era of human discovery, where the number of unanswered questions remains high and the tools required to probe them are finally being built.

Among these questions, general artificial intelligence now has most of its necessary ingredients and can plausibly be developed within a decade. It will accelerate the rate of discovery faster than ever before and push us into a new intelligence revolution. It will bring opportunities for the most incredible benefits across society, yet it will likely also be the most destabilizing and dangerous technology ever created.

Within a short period of time, the majority of science needs to shift toward designing, engineering, and deploying effective and safe intelligence. This requires new neural architectures, wide international cooperation, and appropriate government regulation. The choices we make now will lead to a society filled with either flourishing and happiness, decline and suffering, or even none at all.

Academics
======
Currently, I am a final-year PhD Candidate at the [Computer Systems Laboratory](https://www.csl.cornell.edu/) at Cornell University, under the supervision of [Prof. Zhiru Zhang](https://www.csl.cornell.edu/~zhiruz/index.html) and co-advised by [Mohamed Abdelfattah](https://www.mohsaied.com/). I previously studied a combination of Physics and Computer Science in the College of Arts and Sciences at Cornell University. My academic research currently focuses on building more efficient deep learning systems through efficient neural architectures, low-precision quantization, and dynamic sparsity.


Publications
======

- **Learning from Students: Applying T-Distributions to Explore Accurate and Efficient Formats for LLMs <a href="https://arxiv.org/abs/2405.03103"><img src="https://img.icons8.com/ios-filled/50/000000/pdf.png" alt="PDF" width="16" height="16"></a>**  
  J. Dotzel, Y. Chen, B. Kotb, S. Prasad, G. Wu, S. Li, M. S. Abdelfattah, Z. Zhang  
  *International Conference on Machine Learning, 2024*  

- **FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search <a href="https://arxiv.org/abs/2308.03290"><img src="https://img.icons8.com/ios-filled/50/000000/pdf.png" alt="PDF" width="16" height="16"></a>**  
  J. Dotzel, G. Wu, A. Li, M. Umar, Y. Ni, M. S. Abdelfattah, Z. Zhang, L. Cheng, N. Jouppi, Q. Le, S. Li 
  *International Conference on Automated Machine Learning, 2024*  

- **Exploring the Limits of Semantic Image Compression at Micro-Bits per Pixel <a href="https://arxiv.org/abs/2402.13536"><img src="https://img.icons8.com/ios-filled/50/000000/pdf.png" alt="PDF" width="16" height="16"></a>**  
  J. Dotzel, B. Kotb, J. Dotzel, M. S. Abdelfattah, Z. Zhang  
  *The Second Tiny Papers Track at ICLR, 2024*  

- **Opportunities for Post-Training Dynamic Layer Sparsity in Large Vision and Language Models <a href="https://arxiv.org/abs/2404.04900"><img src="https://img.icons8.com/ios-filled/50/000000/pdf.png" alt="PDF" width="16" height="16"></a>**  
  J. Dotzel, C. Jiang, M. Abdelfattah, Z. Zhang  
  *Efficient Large Vision Models Workshop at CVPR, 2024*

- **M4BRAM: Mixed-Precision Matrix-Matrix Multiplication in FPGA Block RAMs <a href="https://arxiv.org/abs/2311.02758"><img src="https://img.icons8.com/ios-filled/50/000000/pdf.png" alt="PDF" width="16" height="16"></a>**  
  Y. Chen, J. Dotzel, M. S. Abdelfattah  
  *2023 International Conference on Field Programmable Technology, 2023*  

- **Logic Synthesis Meets Machine Learning: Trading Exactness for Generalization <a href="https://arxiv.org/abs/2012.02530"><img src="https://img.icons8.com/ios-filled/50/000000/pdf.png" alt="PDF" width="16" height="16"></a>**  
  S. Rai, W. L. Neto, ..., Y. Zhou, Y. Zhang, J. Dotzel, Z. Zhang, ...  
  *2021 Design, Automation & Test in Europe Conference & Exhibition, 2021*

- **Enabling Design Methodologies and Future Trends for Edge AI: Specialization and Co-Design <a href="https://arxiv.org/abs/2103.15750"><img src="https://img.icons8.com/ios-filled/50/000000/pdf.png" alt="PDF" width="16" height="16"></a>**  
  C. Hao, J. Dotzel, J. Xiong, L. Benini, Z. Zhang, D. Chen  
  *IEEE Design & Test, 2021*

- **Improving Neural Network Quantization Without Retraining Using Outlier Channel Splitting <a href="https://arxiv.org/abs/1901.09504"><img src="https://img.icons8.com/ios-filled/50/000000/pdf.png" alt="PDF" width="16" height="16"></a>**  
  R. Zhao, Y. Hu, J. Dotzel, C. De Sa, Z. Zhang  
  *International Conference on Machine Learning, 2019*  

- **Building Efficient Deep Neural Networks With Unitary Group Convolutions <a href="https://arxiv.org/abs/1811.07755"><img src="https://img.icons8.com/ios-filled/50/000000/pdf.png" alt="PDF" width="16" height="16"></a>**  
  R. Zhao, Y. Hu, J. Dotzel, C. D. Sa, Z. Zhang  
  *Conference on Computer Vision and Pattern Recognition, 2019*

- **OverQ: Opportunistic Outlier Quantization for Neural Network Accelerators <a href="https://arxiv.org/abs/1910.06909"><img src="https://img.icons8.com/ios-filled/50/000000/pdf.png" alt="PDF" width="16" height="16"></a>**  
  J. Dotzel*, R. Zhao*, Z. Hu, P. Ivanov, C. De Sa, Z. Zhang  
  *arXiv, 2019*  


Experience
======

- **Google, TPU Performance Team**  
  *Student Researcher*  
  *June 2024 - Present*  

- **Google, Platforms-Aware AutoML**  
  *Student Researcher*  
  *June 2022 - May 2024*  

- **Computer Systems Laboratory, Cornell**  
*PhD Candidate*  
*August 2019 - Present*  

- **Datto**  
*Software Engineer*  
*June 2017 - Jun 2018*  
